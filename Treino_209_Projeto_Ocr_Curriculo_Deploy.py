import streamlit as st
import requests
from io import BytesIO
from google import genai
from google.genai import types
from google.auth import default
from google.auth.exceptions import DefaultCredentialsError
import json
import os

# Recupera a chave de servi√ßo do secrets.toml
service_key_json = st.secrets["key"]["service_key"]

# Converte o JSON para um dicion√°rio
service_key_dict = json.loads(service_key_json)

# Salva a chave de servi√ßo em um arquivo tempor√°rio
service_key_path = "gcp_service_key.json"  # Caminho tempor√°rio no Streamlit Cloud

with open(service_key_path, "w") as f:
    json.dump(service_key_dict, f)

# Configura a vari√°vel de ambiente para o Google Cloud usar o arquivo
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = service_key_path

try:
    credentials, project = default()
except DefaultCredentialsError:
    st.error("Erro: Credenciais n√£o encontradas. Verifique se configurou corretamente.")
    st.stop()

def generate(text):
    client = genai.Client(
        vertexai=True,
        project="marcos-estudos",
        location="global",
    )

    si_text1 = """Voc√™ deve atuar como um assistente inteligente especializado em an√°lise de curr√≠culos e apresenta√ß√£o profissional. Seu objetivo √© responder √†s perguntas com precis√£o, clareza e profissionalismo, 
            garantindo que a resposta gere uma impress√£o positiva sobre Marcos Bernardino.
            O documento de refer√™ncia √© o curr√≠culo de Marcos Bernardino, que cont√©m informa√ß√µes detalhadas sobre sua trajet√≥ria profissional, habilidades t√©cnicas e interpessoais, certifica√ß√µes, 
            forma√ß√µes acad√™micas, experi√™ncias relevantes e projetos desenvolvidos.
            Como estruturar suas respostas:
            Forne√ßa respostas claras e objetivas, sempre destacando as compet√™ncias, realiza√ß√µes e diferenciais de Marcos.
            Use uma abordagem persuasiva e profissional, ressaltando pontos fortes e experi√™ncias relevantes.
            Quando aplic√°vel, mencione exemplos pr√°ticos ou resultados obtidos para refor√ßar a credibilidade das informa√ß√µes.
            Se uma informa√ß√£o espec√≠fica n√£o estiver dispon√≠vel no documento, responda de forma diplom√°tica e reforce as qualifica√ß√µes gerais de Marcos.
            Pontos-chave a serem valorizados:
            Experi√™ncia Profissional: Destacar cargos, responsabilidades e conquistas.
            Habilidades T√©cnicas: Evidenciar conhecimentos em ferramentas, linguagens e metodologias.
            Certifica√ß√µes e Forma√ß√£o Acad√™mica: Mencionar diplomas e cursos que reforcem a credibilidade.
            Projetos e Iniciativas: Citar trabalhos not√°veis ou contribui√ß√µes relevantes.
            Soft Skills: Ressaltar habilidades interpessoais e diferenciais no ambiente de trabalho.
            Seu tom de resposta deve ser profissional, por√©m acess√≠vel e envolvente. Mantenha um equil√≠brio entre objetividade e entusiasmo para tornar a comunica√ß√£o clara e impactante.
            Ao informar links, mantenha os underlines que est√£o nos links"""
    
    model = "gemini-2.0-flash-001"
    
    # Construindo o contexto da conversa a partir do hist√≥rico
    contents = []
    for message in st.session_state.messages:
        role = "user" if message["role"] == "user" else "model"
        contents.append(types.Content(role=role, parts=[types.Part.from_text(text=message["content"])]))

    # Adiciona a pergunta atual
    contents.append(types.Content(role="user", parts=[types.Part.from_text(text=prompt)]))

    tools = [
        types.Tool(retrieval=types.Retrieval(vertex_ai_search=types.VertexAISearch(
            datastore=st.secrets["google_cloud"]["datastore"]
        )))]
    
    generate_content_config = types.GenerateContentConfig(
        temperature=0.2,
        top_p=0.95,
        max_output_tokens=8192,
        response_modalities=["TEXT"],
        tools=tools,
        system_instruction=[types.Part.from_text(text=instruction)],
    )

    response = client.models.generate_content(
        model=model,
        contents=contents,  # üîπ Agora o Gemini recebe TODO o hist√≥rico
        config=generate_content_config,
    )

    if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:
        return response.candidates[0].content.parts[0].text
    else:
        return "Resposta n√£o encontrada."

# --- Sidebar ---
with st.sidebar:
    # Exibir imagem
    image_url = st.secrets["google_drive"]["image_url"]
    try:
        response = requests.get(image_url)
        image = BytesIO(response.content)
        st.image(image, width=1080)
    except Exception:
        st.error("Erro ao carregar a imagem.")

    # Nome e link para download do CV
    st.markdown("<h4 style='text-align: center; font-size: 24px;'><b>Marcos Bernardino</b></h4>", unsafe_allow_html=True)
    cv_url = st.secrets["google_drive"]["cv_url"]
    st.markdown(f"<p style='text-align: center;'><a href='{cv_url}' download>Download CV</a></p>", unsafe_allow_html=True)
    
    st.markdown("""
    <style>
        #MainMenu {visibility: hidden;}
        header {visibility: hidden;}
        footer {visibility: hidden;}
    </style>
    """, unsafe_allow_html=True)

# --- Inicializar hist√≥rico do chat ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- Exibir t√≠tulo e descri√ß√£o apenas na primeira intera√ß√£o ---
if len(st.session_state.messages) == 0:
    st.title("Marcos Bernardino")
    st.write("Fa√ßa perguntas sobre a vida profissional ou acad√™mica de Marcos Bernardino")

# --- Exibir mensagens do hist√≥rico ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Input do usu√°rio ---
if prompt := st.chat_input("Digite sua pergunta"):
    # Adicionar mensagem do usu√°rio ao hist√≥rico
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Gerar resposta da IA
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        try:
            ai_response = generate(prompt)
            full_response = ai_response
        except Exception as e:
            full_response = f"Ocorreu um erro ao gerar a resposta: {e}"
            st.error(full_response)

        # Exibir resposta e atualizar hist√≥rico
        message_placeholder.markdown(full_response)
        st.session_state.messages.append({"role": "assistant", "content": full_response})
